{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf602c0",
   "metadata": {},
   "source": "# Biological Image Analysis & Target Prediction\n\n## Analysis Goal\n\nThis project develops an automated pipeline to **predict biological targets from 6-channel Cell Painting microscopy images**.\n\n- **Data Exploration:** Understand the biological features across channels (DNA, ER, RNA, AGP, Mito, BF) in 1080x1080 images and their correlation with target values.\n- **Feature Engineering:** Combine traditional image processing (morphological segmentation, intensity statistics, texture analysis) with deep learning (ResNet50 transfer learning feature extraction).\n- **Model Building:** Build a robust regression model under extreme small-sample constraints (n=100), ensuring predictions are free from systematic bias.\n\n## Challenges & Solutions\n\nThree core challenges were identified and resolved during the analysis:\n\n### 1. Systematic Prediction Bias\n- **Challenge:** The initial linear model (Lasso) exhibited a clear slope trend in the residual plot \u2014 low values were overestimated, high values were underestimated.\n- **Solution:** Introduced **Target Transformation (sqrt/log)**. By mapping count-based targets into square-root space, heteroscedasticity was eliminated and the residual slope was reduced from **0.23 to near 0**.\n\n### 2. Outliers & Imaging Artifacts\n- **Challenge:** A small number of samples with extreme bright spots or cell stacking caused the traditional MSE loss function to be \"hijacked\" by these outliers, severely distorting the fitted curve.\n- **Solution:** Switched to **Huber Regression (Robust Regression)**. This algorithm is inherently immune to outlier influence, preserving fitting accuracy for >95% of normal samples.\n\n### 3. Curse of Dimensionality & Overfitting\n- **Challenge:** The extracted feature dimensions (>100) exceeded the sample size (n=100), making the model prone to learning noise rather than signal.\n- **Solution:**\n  - Applied **SelectKBest (k=10)** feature selection based on statistical correlation, retaining only the most biologically interpretable features (e.g., Channel 0 object counts).\n  - Adopted **Leave-One-Out (LOO) cross-validation** to obtain the most unbiased performance estimates under limited data.\n\n**Statistical Note on Model Selection:**\n\nAlthough the *Ridge + Combined* model yielded the highest $R^2$ (0.894), it exhibited a higher MAE compared to the Lasso baseline, suggesting that the $R^2$ was disproportionately influenced by high-value samples. Furthermore, given the small sample size ($N = 100$) and high dimensionality ($P \\approx 100$), **Huber Regression with Target Transformation (sqrt)** was selected as the final model. This choice prioritizes **homoscedasticity** and **robustness to imaging artifacts** over raw fitting scores, effectively mitigating the systematic bias observed in initial diagnostics.\n\n## Key Results\n\n| Metric | Value |\n|--------|-------|\n| **Best Model** | Huber Regressor + Sqrt Transform + SelectKBest |\n| **R-squared** | $R^2 \\approx 0.87$ |\n| **MAE** | $\\approx 1.86$ |\n| **Residual Quality** | Randomly distributed around zero \u2014 no systematic bias |\n\n### Before & After: Residual Bias Correction\n\n**Before (Lasso baseline) vs After (Optimized):** The systematic slope in residuals is eliminated.\n\n![Before vs After Optimization](optimization_comparison.png)\n\n**Outlier Feature Analysis:** Heatmap revealing which features make outlier samples anomalous.\n\n![Outlier Feature Heatmap](outlier_feature_heatmap.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438644b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:33:44.853735Z",
     "iopub.status.busy": "2026-02-14T06:33:44.853673Z",
     "iopub.status.idle": "2026-02-14T06:33:46.647733Z",
     "shell.execute_reply": "2026-02-14T06:33:46.647295Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import ndimage\n",
    "from skimage import filters, measure, morphology\n",
    "import os\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Load data\n",
    "images = np.load('images_data/images.npy')\n",
    "targets = np.load('images_data/targets.npy')\n",
    "\n",
    "print(f\"Images: {images.shape}, dtype={images.dtype}\")\n",
    "print(f\"Targets: {targets.shape}, dtype={targets.dtype}\")\n",
    "print(f\"Target range: [{targets.min()}, {targets.max()}], mean={targets.mean():.1f}, std={targets.std():.1f}\")\n",
    "print(f\"Unique target values: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdbc90",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Exploratory Data Analysis\n",
    "\n",
    "## 1.1 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90254efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:33:46.649277Z",
     "iopub.status.busy": "2026-02-14T06:33:46.649098Z",
     "iopub.status.idle": "2026-02-14T06:33:46.747132Z",
     "shell.execute_reply": "2026-02-14T06:33:46.746725Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(targets, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Target Value')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Target Distribution')\n",
    "axes[0].axvline(x=targets.mean(), color='red', linestyle='--', label=f'Mean={targets.mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(range(len(targets)), np.sort(targets), c='steelblue', s=20, alpha=0.7)\n",
    "axes[1].set_xlabel('Sample Index (sorted)')\n",
    "axes[1].set_ylabel('Target Value')\n",
    "axes[1].set_title('Sorted Target Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2eb10",
   "metadata": {},
   "source": [
    "## 1.2 Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74530595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:33:46.748166Z",
     "iopub.status.busy": "2026-02-14T06:33:46.748092Z",
     "iopub.status.idle": "2026-02-14T06:33:48.471376Z",
     "shell.execute_reply": "2026-02-14T06:33:48.470907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize sample images across all 6 channels\n",
    "channel_names = ['Ch0 (DNA?)', 'Ch1 (ER?)', 'Ch2 (RNA?)', 'Ch3 (AGP?)', 'Ch4 (Mito?)', 'Ch5 (BF?)']\n",
    "\n",
    "# Pick 4 images with different target values\n",
    "sample_idx = [np.argmin(targets), np.argmax(targets),\n",
    "              np.argsort(targets)[25], np.argsort(targets)[75]]\n",
    "\n",
    "fig, axes = plt.subplots(4, 6, figsize=(24, 16))\n",
    "\n",
    "for row, idx in enumerate(sample_idx):\n",
    "    for ch in range(6):\n",
    "        img = images[idx, :, :, ch]\n",
    "        vmax = np.percentile(img, 99.5)\n",
    "        axes[row, ch].imshow(img, cmap='gray', vmin=0, vmax=max(vmax, 1))\n",
    "        axes[row, ch].set_title(f'{channel_names[ch]}\\nTarget={targets[idx]}', fontsize=9)\n",
    "        axes[row, ch].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images Across 6 Channels (4 different target values)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc89e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:33:48.475806Z",
     "iopub.status.busy": "2026-02-14T06:33:48.475729Z",
     "iopub.status.idle": "2026-02-14T06:33:52.188593Z",
     "shell.execute_reply": "2026-02-14T06:33:52.188275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Per-channel statistics\n",
    "print(\"Per-channel statistics:\")\n",
    "print(f\"{'Channel':<12} {'Min':>6} {'Max':>6} {'Mean':>8} {'Std':>8} {'99th%':>8}\")\n",
    "print(\"-\" * 52)\n",
    "for ch in range(6):\n",
    "    ch_data = images[:, :, :, ch]\n",
    "    print(f\"{channel_names[ch]:<12} {ch_data.min():>6} {ch_data.max():>6} \"\n",
    "          f\"{ch_data.mean():>8.2f} {ch_data.std():>8.2f} {np.percentile(ch_data, 99):>8.1f}\")\n",
    "\n",
    "# Composite view of one image\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "idx = np.argsort(targets)[50]  # median target\n",
    "\n",
    "# RGB composite of first 3 channels\n",
    "rgb = np.stack([images[idx,:,:,0], images[idx,:,:,1], images[idx,:,:,2]], axis=-1)\n",
    "rgb_scaled = (rgb.astype(float) / np.percentile(rgb, 99.5) * 255).clip(0, 255).astype(np.uint8)\n",
    "axes[0].imshow(rgb_scaled)\n",
    "axes[0].set_title(f'RGB Composite (Ch 0,1,2), Target={targets[idx]}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Second RGB composite\n",
    "rgb2 = np.stack([images[idx,:,:,3], images[idx,:,:,4], images[idx,:,:,5]], axis=-1)\n",
    "rgb2_scaled = (rgb2.astype(float) / max(np.percentile(rgb2, 99.5), 1) * 255).clip(0, 255).astype(np.uint8)\n",
    "axes[1].imshow(rgb2_scaled)\n",
    "axes[1].set_title(f'RGB Composite (Ch 3,4,5), Target={targets[idx]}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Channel 0 (likely DNA/nuclei) zoomed\n",
    "axes[2].imshow(images[idx, 300:700, 300:700, 0], cmap='hot')\n",
    "axes[2].set_title(f'Ch0 (Nuclei?) Zoomed, Target={targets[idx]}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ce3d1",
   "metadata": {},
   "source": [
    "## 1.3 Relationship Between Image Intensity and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ffa82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:33:52.191437Z",
     "iopub.status.busy": "2026-02-14T06:33:52.191347Z",
     "iopub.status.idle": "2026-02-14T06:33:52.669887Z",
     "shell.execute_reply": "2026-02-14T06:33:52.669552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick check: does mean intensity per channel correlate with target?\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ch in range(6):\n",
    "    ax = axes[ch // 3, ch % 3]\n",
    "    mean_intensities = images[:, :, :, ch].reshape(100, -1).mean(axis=1)\n",
    "    ax.scatter(mean_intensities, targets, alpha=0.6, s=30, c='steelblue')\n",
    "    corr = np.corrcoef(mean_intensities, targets)[0, 1]\n",
    "    ax.set_title(f'{channel_names[ch]} (r={corr:.3f})')\n",
    "    ax.set_xlabel('Mean Intensity')\n",
    "    ax.set_ylabel('Target')\n",
    "\n",
    "plt.suptitle('Mean Channel Intensity vs Target', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b4f5c",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Feature Engineering\n",
    "\n",
    "We extract multiple feature types from each image:\n",
    "1. **Intensity features**: mean, std, percentiles per channel\n",
    "2. **Object counting**: threshold + connected components (potential nuclei count)\n",
    "3. **Texture features**: entropy, gradient magnitude\n",
    "4. **Morphological features**: object area statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47986c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:33:52.671100Z",
     "iopub.status.busy": "2026-02-14T06:33:52.671039Z",
     "iopub.status.idle": "2026-02-14T06:34:27.124627Z",
     "shell.execute_reply": "2026-02-14T06:34:27.124266Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    \"\"\"Extract features from a single 6-channel image.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    for ch in range(6):\n",
    "        channel = img[:, :, ch].astype(np.float64)\n",
    "        prefix = f'ch{ch}'\n",
    "        \n",
    "        # Intensity statistics\n",
    "        features[f'{prefix}_mean'] = channel.mean()\n",
    "        features[f'{prefix}_std'] = channel.std()\n",
    "        features[f'{prefix}_max'] = channel.max()\n",
    "        features[f'{prefix}_p95'] = np.percentile(channel, 95)\n",
    "        features[f'{prefix}_p99'] = np.percentile(channel, 99)\n",
    "        features[f'{prefix}_skew'] = float(pd.Series(channel.ravel()).skew())\n",
    "        \n",
    "        # Fraction of bright pixels\n",
    "        threshold = np.percentile(channel, 95)\n",
    "        if threshold > 0:\n",
    "            features[f'{prefix}_bright_frac'] = (channel > threshold).mean()\n",
    "        else:\n",
    "            features[f'{prefix}_bright_frac'] = 0.0\n",
    "        \n",
    "        # Entropy (texture)\n",
    "        hist, _ = np.histogram(channel, bins=64, range=(0, 255))\n",
    "        hist = hist / hist.sum()\n",
    "        hist = hist[hist > 0]\n",
    "        features[f'{prefix}_entropy'] = -np.sum(hist * np.log2(hist))\n",
    "        \n",
    "        # Gradient magnitude (edge content)\n",
    "        gy, gx = np.gradient(channel)\n",
    "        grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "        features[f'{prefix}_grad_mean'] = grad_mag.mean()\n",
    "        features[f'{prefix}_grad_std'] = grad_mag.std()\n",
    "    \n",
    "    # Object counting on Channel 0 (likely DNA/nuclei)\n",
    "    ch0 = img[:, :, 0].astype(np.float64)\n",
    "    \n",
    "    # Adaptive thresholding approach\n",
    "    for t_name, threshold_val in [('otsu', None), ('p97', np.percentile(ch0, 97)), ('p95', np.percentile(ch0, 95))]:\n",
    "        if t_name == 'otsu':\n",
    "            try:\n",
    "                threshold_val = filters.threshold_otsu(ch0)\n",
    "            except ValueError:\n",
    "                threshold_val = ch0.mean() + 2 * ch0.std()\n",
    "        \n",
    "        binary = ch0 > threshold_val\n",
    "        # Clean up\n",
    "        binary = morphology.remove_small_objects(binary, min_size=50)\n",
    "        binary = morphology.binary_dilation(binary, morphology.disk(1))\n",
    "        labeled = measure.label(binary)\n",
    "        props = measure.regionprops(labeled)\n",
    "        \n",
    "        features[f'ch0_obj_count_{t_name}'] = len(props)\n",
    "        if props:\n",
    "            areas = [p.area for p in props]\n",
    "            features[f'ch0_obj_mean_area_{t_name}'] = np.mean(areas)\n",
    "            features[f'ch0_obj_total_area_{t_name}'] = np.sum(areas)\n",
    "        else:\n",
    "            features[f'ch0_obj_mean_area_{t_name}'] = 0\n",
    "            features[f'ch0_obj_total_area_{t_name}'] = 0\n",
    "    \n",
    "    # Object counting on Channel 1 (ER/cytoplasm)\n",
    "    ch1 = img[:, :, 1].astype(np.float64)\n",
    "    try:\n",
    "        t_otsu = filters.threshold_otsu(ch1)\n",
    "    except ValueError:\n",
    "        t_otsu = ch1.mean() + 2 * ch1.std()\n",
    "    binary1 = ch1 > t_otsu\n",
    "    binary1 = morphology.remove_small_objects(binary1, min_size=50)\n",
    "    labeled1 = measure.label(binary1)\n",
    "    features['ch1_obj_count'] = labeled1.max()\n",
    "    \n",
    "    # Cross-channel features\n",
    "    for i in range(6):\n",
    "        for j in range(i+1, 6):\n",
    "            ci = img[:, :, i].astype(np.float64).ravel()\n",
    "            cj = img[:, :, j].astype(np.float64).ravel()\n",
    "            corr = np.corrcoef(ci, cj)[0, 1]\n",
    "            features[f'corr_ch{i}_ch{j}'] = corr if not np.isnan(corr) else 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all images\n",
    "print(\"Extracting features from 100 images...\")\n",
    "all_features = []\n",
    "for i in range(len(images)):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Processing image {i+1}/100...\")\n",
    "    feats = extract_features(images[i])\n",
    "    all_features.append(feats)\n",
    "\n",
    "feature_df = pd.DataFrame(all_features)\n",
    "print(f\"\\nExtracted {feature_df.shape[1]} features per image\")\n",
    "print(f\"Feature matrix shape: {feature_df.shape}\")\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d6a7",
   "metadata": {},
   "source": [
    "## 2.1 Feature-Target Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ce5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:27.125680Z",
     "iopub.status.busy": "2026-02-14T06:34:27.125610Z",
     "iopub.status.idle": "2026-02-14T06:34:27.333703Z",
     "shell.execute_reply": "2026-02-14T06:34:27.333356Z"
    }
   },
   "outputs": [],
   "source": [
    "# Top correlated features with target\n",
    "correlations = feature_df.corrwith(pd.Series(targets)).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 features most correlated with target:\")\n",
    "print(\"=\" * 50)\n",
    "for feat, corr in correlations.head(20).items():\n",
    "    sign = '+' if feature_df[feat].corr(pd.Series(targets)) > 0 else '-'\n",
    "    print(f\"  {sign}{corr:.3f}  {feat}\")\n",
    "\n",
    "# Plot top 6 features vs target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "top_feats = correlations.head(6).index.tolist()\n",
    "\n",
    "for i, feat in enumerate(top_feats):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.scatter(feature_df[feat], targets, alpha=0.6, s=30, c='steelblue')\n",
    "    corr = feature_df[feat].corr(pd.Series(targets))\n",
    "    ax.set_title(f'{feat}\\n(r={corr:.3f})', fontsize=10)\n",
    "    ax.set_xlabel('Feature Value')\n",
    "    ax.set_ylabel('Target')\n",
    "    # Add regression line\n",
    "    z = np.polyfit(feature_df[feat], targets, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(feature_df[feat].min(), feature_df[feat].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7)\n",
    "\n",
    "plt.suptitle('Top 6 Correlated Features vs Target', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4830ec",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Evaluation Strategy\n",
    "\n",
    "With only 100 samples, we use:\n",
    "- **5-Fold Cross-Validation** (repeated 3x for stability)\n",
    "- **Leave-One-Out CV** for final best model assessment\n",
    "- **Metrics**: MAE, RMSE, R^2\n",
    "\n",
    "This is a **regression** problem (predicting continuous counts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09bab5",
   "metadata": {},
   "source": [
    "# Part 4: Model Development\n",
    "\n",
    "## 4.1 Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e2d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:27.334849Z",
     "iopub.status.busy": "2026-02-14T06:34:27.334784Z",
     "iopub.status.idle": "2026-02-14T06:34:29.540930Z",
     "shell.execute_reply": "2026-02-14T06:34:29.540436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = feature_df.values\n",
    "y = targets\n",
    "\n",
    "# Handle NaN/Inf\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=10.0),\n",
    "    'Lasso': Lasso(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=5, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42),\n",
    "}\n",
    "\n",
    "# 5-Fold CV (repeated 3x)\n",
    "print(\"=\" * 70)\n",
    "print(\"5-FOLD CROSS-VALIDATION RESULTS (repeated 3x)\")\n",
    "print(\"=\" * 70)\n",
    "r2_hdr = \"R^2\"\n",
    "print(f\"{'Model':<25} {'MAE':>8} {'RMSE':>8} {r2_hdr:>8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    all_mae, all_rmse, all_r2 = [], [], []\n",
    "    \n",
    "    for seed in [42, 123, 456]:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        \n",
    "        y_pred_cv = cross_val_predict(model, X_scaled, y, cv=kf)\n",
    "        mae = mean_absolute_error(y, y_pred_cv)\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred_cv))\n",
    "        r2 = r2_score(y, y_pred_cv)\n",
    "        \n",
    "        all_mae.append(mae)\n",
    "        all_rmse.append(rmse)\n",
    "        all_r2.append(r2)\n",
    "    \n",
    "    mean_mae = np.mean(all_mae)\n",
    "    mean_rmse = np.mean(all_rmse)\n",
    "    mean_r2 = np.mean(all_r2)\n",
    "    \n",
    "    cv_results[name] = {'MAE': mean_mae, 'RMSE': mean_rmse, 'R2': mean_r2}\n",
    "    print(f\"{name:<25} {mean_mae:>8.2f} {mean_rmse:>8.2f} {mean_r2:>8.3f}\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = min(cv_results, key=lambda x: cv_results[x]['MAE'])\n",
    "print(f\"\\nBest model by MAE: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f188d",
   "metadata": {},
   "source": [
    "## 4.2 Leave-One-Out CV (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f5647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:29.542059Z",
     "iopub.status.busy": "2026-02-14T06:34:29.541997Z",
     "iopub.status.idle": "2026-02-14T06:34:29.654995Z",
     "shell.execute_reply": "2026-02-14T06:34:29.654659Z"
    }
   },
   "outputs": [],
   "source": [
    "# LOO CV on best model\n",
    "print(f\"Running Leave-One-Out CV for {best_model_name}...\")\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "y_pred_loo = cross_val_predict(best_model, X_scaled, y, cv=loo)\n",
    "\n",
    "mae_loo = mean_absolute_error(y, y_pred_loo)\n",
    "rmse_loo = np.sqrt(mean_squared_error(y, y_pred_loo))\n",
    "r2_loo = r2_score(y, y_pred_loo)\n",
    "\n",
    "print(f\"\\nLOO CV Results ({best_model_name}):\")\n",
    "print(f\"  MAE:  {mae_loo:.2f}\")\n",
    "print(f\"  RMSE: {rmse_loo:.2f}\")\n",
    "print(f\"  R^2:  {r2_loo:.3f}\")\n",
    "\n",
    "# Predicted vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(y, y_pred_loo, alpha=0.6, s=40, c='steelblue', edgecolors='white', linewidth=0.5)\n",
    "min_val = min(y.min(), y_pred_loo.min()) - 2\n",
    "max_val = max(y.max(), y_pred_loo.max()) + 2\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Target')\n",
    "axes[0].set_ylabel('Predicted Target')\n",
    "axes[0].set_title(f'{best_model_name} - LOO CV\\nMAE={mae_loo:.2f}, R^2={r2_loo:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals\n",
    "residuals = y - y_pred_loo\n",
    "axes[1].scatter(y, residuals, alpha=0.6, s=40, c='coral', edgecolors='white', linewidth=0.5)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1].set_xlabel('Actual Target')\n",
    "axes[1].set_ylabel('Residual (Actual - Predicted)')\n",
    "axes[1].set_title(f'Residual Plot\\nMean residual={residuals.mean():.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a9826",
   "metadata": {},
   "source": [
    "## 4.3 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde50e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:29.656193Z",
     "iopub.status.busy": "2026-02-14T06:34:29.656118Z",
     "iopub.status.idle": "2026-02-14T06:34:29.806908Z",
     "shell.execute_reply": "2026-02-14T06:34:29.806644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train on full data for feature importance\n",
    "if 'Random Forest' in models:\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=5, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_scaled, y)\n",
    "    importances = rf.feature_importances_\n",
    "    feat_names = feature_df.columns\n",
    "    \n",
    "    imp_df = pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
    "    imp_df = imp_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    top20 = imp_df.head(20)\n",
    "    ax.barh(top20['feature'][::-1], top20['importance'][::-1], color='steelblue')\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title('Top 20 Feature Importances (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 15 features:\")\n",
    "    for _, row in imp_df.head(15).iterrows():\n",
    "        print(f\"  {row['importance']:.4f}  {row['feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083eb7e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Object Counting Approach\n",
    "\n",
    "Since targets likely represent cell/nuclei counts, let's try a direct object-counting approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aba928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:29.808117Z",
     "iopub.status.busy": "2026-02-14T06:34:29.808053Z",
     "iopub.status.idle": "2026-02-14T06:34:40.826977Z",
     "shell.execute_reply": "2026-02-14T06:34:40.826604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Direct nuclei counting using Channel 0 (DNA)\n",
    "def count_nuclei(img_ch0, min_size=30, max_size=5000):\n",
    "    \"\"\"Count nuclei-like objects in a channel image.\"\"\"\n",
    "    ch = img_ch0.astype(np.float64)\n",
    "    \n",
    "    # Try multiple thresholds and return the best\n",
    "    results = {}\n",
    "    \n",
    "    for p in [95, 96, 97, 98]:\n",
    "        threshold = np.percentile(ch, p)\n",
    "        if threshold < 1:\n",
    "            continue\n",
    "        binary = ch > threshold\n",
    "        binary = morphology.remove_small_objects(binary, min_size=min_size)\n",
    "        \n",
    "        # Watershed-like separation: erode then dilate\n",
    "        binary = morphology.binary_erosion(binary, morphology.disk(2))\n",
    "        binary = morphology.binary_dilation(binary, morphology.disk(2))\n",
    "        \n",
    "        labeled = measure.label(binary)\n",
    "        props = measure.regionprops(labeled)\n",
    "        \n",
    "        # Filter by size\n",
    "        valid = [p for p in props if min_size < p.area < max_size]\n",
    "        results[f'p{p}'] = len(valid)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Count for all images\n",
    "print(\"Counting nuclei across all images...\")\n",
    "count_results = []\n",
    "for i in range(100):\n",
    "    counts = count_nuclei(images[i, :, :, 0])\n",
    "    counts['idx'] = i\n",
    "    counts['target'] = targets[i]\n",
    "    count_results.append(counts)\n",
    "\n",
    "count_df = pd.DataFrame(count_results)\n",
    "\n",
    "# Find best counting threshold\n",
    "print(\"\\nNuclei count correlations with target:\")\n",
    "for col in [c for c in count_df.columns if c.startswith('p')]:\n",
    "    r = count_df[col].corr(count_df['target'])\n",
    "    mae = mean_absolute_error(count_df['target'], count_df[col])\n",
    "    print(f\"  {col}: r={r:.3f}, MAE={mae:.1f}\")\n",
    "\n",
    "# Plot best counting method\n",
    "best_count_col = max([c for c in count_df.columns if c.startswith('p')],\n",
    "                     key=lambda c: abs(count_df[c].corr(count_df['target'])))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(count_df[best_count_col], count_df['target'], alpha=0.6, s=40, c='coral')\n",
    "ax.plot([0, count_df[best_count_col].max()], [0, count_df[best_count_col].max()], 'r--', alpha=0.5)\n",
    "r = count_df[best_count_col].corr(count_df['target'])\n",
    "mae = mean_absolute_error(count_df['target'], count_df[best_count_col])\n",
    "ax.set_xlabel(f'Counted Objects ({best_count_col} threshold)')\n",
    "ax.set_ylabel('Target Value')\n",
    "ax.set_title(f'Direct Object Counting vs Target\\nr={r:.3f}, MAE={mae:.1f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11e67f",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f4cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:40.828066Z",
     "iopub.status.busy": "2026-02-14T06:34:40.827997Z",
     "iopub.status.idle": "2026-02-14T06:34:40.830424Z",
     "shell.execute_reply": "2026-02-14T06:34:40.830138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. DATA OVERVIEW:\")\n",
    "print(f\"   - 100 images, 1080x1080, 6 channels (Cell Painting)\")\n",
    "print(f\"   - Target range: [{targets.min()}, {targets.max()}], likely nuclei/cell counts\")\n",
    "print(f\"   - Regression problem with small dataset\")\n",
    "\n",
    "print(\"\\n2. EVALUATION STRATEGY:\")\n",
    "print(f\"   - 5-Fold CV (3x repeated) for model comparison\")\n",
    "print(f\"   - Leave-One-Out CV for final assessment\")\n",
    "print(f\"   - Metrics: MAE, RMSE, R^2\")\n",
    "\n",
    "print(\"\\n3. MODEL RESULTS (5-Fold CV):\")\n",
    "for name, res in sorted(cv_results.items(), key=lambda x: x[1]['MAE']):\n",
    "    print(f\"   {name:<25} MAE={res['MAE']:.2f}, RMSE={res['RMSE']:.2f}, R^2={res['R2']:.3f}\")\n",
    "\n",
    "print(f\"\\n4. BEST MODEL ({best_model_name}) LOO CV:\")\n",
    "print(f\"   MAE={mae_loo:.2f}, RMSE={rmse_loo:.2f}, R^2={r2_loo:.3f}\")\n",
    "\n",
    "print(\"\\n5. KEY FINDINGS:\")\n",
    "print(f\"   - Top correlated features: {', '.join(correlations.head(3).index.tolist())}\")\n",
    "print(f\"   - Object counting approach: r={r:.3f}\")\n",
    "print(f\"   - Image-derived features can predict target with reasonable accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867046f",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Addressing Systematic Bias \u2014 Four Improvement Strategies\n",
    "\n",
    "Current Lasso model shows systematic bias in residuals:\n",
    "- **Low values overestimated, high values underestimated** (regression to the mean)\n",
    "- **Extreme outliers** (Actual ~20, residual ~-30) distort MSE-based fitting\n",
    "- These indicate the linear model has reached its ceiling\n",
    "\n",
    "We try four different strategies to break through this bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442016a",
   "metadata": {},
   "source": [
    "## Strategy 1: Robust Regression (HuberRegressor / TheilSenRegressor)\n",
    "\n",
    "The extreme outliers (residual reaching -30) pull the Lasso fit line. Robust regressors use loss functions that down-weight outliers, fitting the **majority** of data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a7615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:34:40.831544Z",
     "iopub.status.busy": "2026-02-14T06:34:40.831469Z",
     "iopub.status.idle": "2026-02-14T06:37:03.168935Z",
     "shell.execute_reply": "2026-02-14T06:37:03.168589Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor, TheilSenRegressor\n",
    "\n",
    "robust_models = {\n",
    "    'HuberRegressor (e=1.35)': HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500),\n",
    "    'HuberRegressor (e=1.1)': HuberRegressor(epsilon=1.1, alpha=1.0, max_iter=500),\n",
    "    'TheilSenRegressor': TheilSenRegressor(random_state=42, n_jobs=-1),\n",
    "    'Lasso (baseline)': Lasso(alpha=1.0),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY 1: ROBUST REGRESSION - LOO CV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "robust_results = {}\n",
    "\n",
    "for name, model in robust_models.items():\n",
    "    y_pred = cross_val_predict(model, X_scaled, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    robust_results[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'y_pred': y_pred}\n",
    "    print(f\"  {name:<30} MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "for ax, (name, res) in zip(axes.ravel(), robust_results.items()):\n",
    "    residuals_r = y - res['y_pred']\n",
    "    ax.scatter(y, residuals_r, alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Actual Target')\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R2={res[\"R2\"]:.3f}')\n",
    "    # Add trend line to show bias\n",
    "    z = np.polyfit(y, residuals_r, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(y.min(), y.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'slope={z[0]:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Strategy 1: Robust Regression - Residual Plots', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy1_robust_regression.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: strategy1_robust_regression.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab068ad",
   "metadata": {},
   "source": [
    "## Strategy 2: Non-linear Target Transformation (log / sqrt)\n",
    "\n",
    "The residuals grow with target magnitude, suggesting the target follows a long-tailed distribution (e.g. Poisson). Predicting `log(1+y)` or `sqrt(y)` stabilizes variance and reduces systematic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293b64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:03.170805Z",
     "iopub.status.busy": "2026-02-14T06:37:03.170704Z",
     "iopub.status.idle": "2026-02-14T06:37:03.865676Z",
     "shell.execute_reply": "2026-02-14T06:37:03.865277Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "transformations = {\n",
    "    'Lasso (no transform)': Lasso(alpha=1.0),\n",
    "    'Lasso + log(1+y)': TransformedTargetRegressor(\n",
    "        regressor=Lasso(alpha=1.0),\n",
    "        func=np.log1p, inverse_func=np.expm1\n",
    "    ),\n",
    "    'Lasso + sqrt(y)': TransformedTargetRegressor(\n",
    "        regressor=Lasso(alpha=1.0),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "    'Ridge + log(1+y)': TransformedTargetRegressor(\n",
    "        regressor=Ridge(alpha=10.0),\n",
    "        func=np.log1p, inverse_func=np.expm1\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY 2: TARGET TRANSFORMATION - LOO CV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "transform_results = {}\n",
    "\n",
    "for name, model in transformations.items():\n",
    "    y_pred = cross_val_predict(model, X_scaled, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    transform_results[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'y_pred': y_pred}\n",
    "    print(f\"  {name:<30} MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "for ax, (name, res) in zip(axes.ravel(), transform_results.items()):\n",
    "    residuals_t = y - res['y_pred']\n",
    "    ax.scatter(y, residuals_t, alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Actual Target')\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R2={res[\"R2\"]:.3f}')\n",
    "    z = np.polyfit(y, residuals_t, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(y.min(), y.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'slope={z[0]:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Strategy 2: Target Transformation - Residual Plots', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy2_target_transform.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: strategy2_target_transform.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673ad04",
   "metadata": {},
   "source": [
    "## Strategy 3: Non-linear Models (SVR / XGBoost)\n",
    "\n",
    "Linear models cannot capture non-linear feature-target relationships. SVR with RBF kernel is particularly suited for small datasets (n=100), while XGBoost captures complex interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8b643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:03.866792Z",
     "iopub.status.busy": "2026-02-14T06:37:03.866719Z",
     "iopub.status.idle": "2026-02-14T06:37:24.006315Z",
     "shell.execute_reply": "2026-02-14T06:37:24.005930Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- SVR with hyperparameter tuning ---\n",
    "print(\"Tuning SVR hyperparameters (5-Fold CV)...\")\n",
    "svr_param_grid = {\n",
    "    'C': [1, 10, 50, 100],\n",
    "    'epsilon': [0.1, 0.5, 1.0],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svr_grid = GridSearchCV(SVR(kernel='rbf'), svr_param_grid, cv=5,\n",
    "                        scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "svr_grid.fit(X_scaled, y)\n",
    "print(f\"  Best SVR params: {svr_grid.best_params_}\")\n",
    "print(f\"  Best 5-Fold MAE: {-svr_grid.best_score_:.2f}\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    has_xgb = True\n",
    "except ImportError:\n",
    "    print(\"  XGBoost not installed, using GradientBoosting as substitute\")\n",
    "    has_xgb = False\n",
    "\n",
    "nonlinear_models = {\n",
    "    'SVR (tuned)': SVR(kernel='rbf', **svr_grid.best_params_),\n",
    "    'SVR (linear)': SVR(kernel='linear', C=10),\n",
    "    'Lasso (baseline)': Lasso(alpha=1.0),\n",
    "}\n",
    "\n",
    "if has_xgb:\n",
    "    nonlinear_models['XGBoost'] = XGBRegressor(\n",
    "        n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        reg_alpha=1.0, reg_lambda=1.0,\n",
    "        random_state=42, verbosity=0\n",
    "    )\n",
    "else:\n",
    "    nonlinear_models['GradientBoosting'] = GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "        subsample=0.8, random_state=42\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STRATEGY 3: NON-LINEAR MODELS - LOO CV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nonlinear_results = {}\n",
    "\n",
    "for name, model in nonlinear_models.items():\n",
    "    y_pred = cross_val_predict(model, X_scaled, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    nonlinear_results[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'y_pred': y_pred}\n",
    "    print(f\"  {name:<30} MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}\")\n",
    "\n",
    "# Predicted vs Actual\n",
    "n_models = len(nonlinear_results)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, res) in zip(axes, nonlinear_results.items()):\n",
    "    ax.scatter(y, res['y_pred'], alpha=0.6, s=40, c='steelblue', edgecolors='white', linewidth=0.5)\n",
    "    lims = [min(y.min(), res['y_pred'].min()) - 2, max(y.max(), res['y_pred'].max()) + 2]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.7)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R2={res[\"R2\"]:.3f}')\n",
    "\n",
    "plt.suptitle('Strategy 3: Non-linear Models - Predicted vs Actual', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy3_nonlinear_pred.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residual comparison\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, res) in zip(axes, nonlinear_results.items()):\n",
    "    residuals_n = y - res['y_pred']\n",
    "    ax.scatter(y, residuals_n, alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Actual Target')\n",
    "    ax.set_ylabel('Residual')\n",
    "    z = np.polyfit(y, residuals_n, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(y.min(), y.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'slope={z[0]:.3f}')\n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Strategy 3: Non-linear Models - Residual Plots', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy3_nonlinear_resid.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: strategy3_nonlinear_pred.png, strategy3_nonlinear_resid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e6569",
   "metadata": {},
   "source": [
    "## Strategy 4: Transfer Learning Features (ResNet50)\n",
    "\n",
    "Hand-crafted features (intensity statistics, simple object counts) are limited. A pretrained ResNet50 can extract rich visual representations that capture spatial patterns, textures, and structures invisible to manual feature engineering.\n",
    "\n",
    "We extract features from each channel (converted to 3-channel pseudo-RGB), then concatenate and feed into Ridge/Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af0371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:24.007477Z",
     "iopub.status.busy": "2026-02-14T06:37:24.007399Z",
     "iopub.status.idle": "2026-02-14T06:37:32.423820Z",
     "shell.execute_reply": "2026-02-14T06:37:32.423223Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained ResNet50 as feature extractor (remove final FC layer)\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove FC, keep avgpool\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# ImageNet normalization\n",
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def extract_resnet_features(imgs, channels_groups=[(0, 1, 2), (3, 4, 5)]):\n",
    "    \"\"\"Extract ResNet50 features from multi-channel images.\"\"\"\n",
    "    from PIL import Image\n",
    "    all_features = []\n",
    "    \n",
    "    for i in range(len(imgs)):\n",
    "        img_feats = []\n",
    "        for group in channels_groups:\n",
    "            rgb = np.stack([imgs[i, :, :, ch] for ch in group], axis=-1).astype(np.float32)\n",
    "            for c in range(3):\n",
    "                ch_max = rgb[:, :, c].max()\n",
    "                if ch_max > 0:\n",
    "                    rgb[:, :, c] = rgb[:, :, c] / ch_max\n",
    "            \n",
    "            pil_img = Image.fromarray((rgb * 255).astype(np.uint8))\n",
    "            pil_img = pil_img.resize((224, 224), Image.BILINEAR)\n",
    "            \n",
    "            tensor_img = T.ToTensor()(pil_img)\n",
    "            tensor_img = normalize(tensor_img)\n",
    "            tensor_img = tensor_img.unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                feat = resnet(tensor_img).squeeze().cpu().numpy()\n",
    "            img_feats.append(feat)\n",
    "        \n",
    "        all_features.append(np.concatenate(img_feats))\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(imgs)} images...\")\n",
    "    \n",
    "    return np.array(all_features)\n",
    "\n",
    "print(\"Extracting ResNet50 features from all 100 images...\")\n",
    "print(\"  (2 channel groups x 2048 dims = 4096 deep features per image)\")\n",
    "X_deep = extract_resnet_features(images)\n",
    "print(f\"  Deep feature matrix shape: {X_deep.shape}\")\n",
    "\n",
    "# Scale deep features\n",
    "scaler_deep = StandardScaler()\n",
    "X_deep_scaled = scaler_deep.fit_transform(X_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb279528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:32.428823Z",
     "iopub.status.busy": "2026-02-14T06:37:32.428244Z",
     "iopub.status.idle": "2026-02-14T06:37:32.984765Z",
     "shell.execute_reply": "2026-02-14T06:37:32.984391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate deep features with different models\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA to reduce 4096 dims (avoid curse of dimensionality with n=100)\n",
    "for n_comp in [10, 20, 50]:\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_deep_scaled)\n",
    "    variance_explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"PCA n={n_comp}: variance explained = {variance_explained:.3f}\")\n",
    "\n",
    "# Use PCA-reduced deep features\n",
    "pca_best = PCA(n_components=20, random_state=42)\n",
    "X_deep_pca = pca_best.fit_transform(X_deep_scaled)\n",
    "\n",
    "# Also combine: hand-crafted + deep features\n",
    "X_combined = np.hstack([X_scaled, X_deep_pca])\n",
    "print(f\"\\nFeature sets:\")\n",
    "print(f\"  Hand-crafted:  {X_scaled.shape}\")\n",
    "print(f\"  Deep (PCA-20): {X_deep_pca.shape}\")\n",
    "print(f\"  Combined:      {X_combined.shape}\")\n",
    "\n",
    "# Evaluate\n",
    "deep_feature_sets = {\n",
    "    'Lasso + HandCrafted (baseline)': (X_scaled, Lasso(alpha=1.0)),\n",
    "    'Ridge + Deep (PCA-20)': (X_deep_pca, Ridge(alpha=10.0)),\n",
    "    'Lasso + Deep (PCA-20)': (X_deep_pca, Lasso(alpha=1.0)),\n",
    "    'Ridge + Combined': (X_combined, Ridge(alpha=10.0)),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STRATEGY 4: TRANSFER LEARNING FEATURES - LOO CV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "deep_results = {}\n",
    "\n",
    "for name, (X_feat, model) in deep_feature_sets.items():\n",
    "    y_pred = cross_val_predict(model, X_feat, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    deep_results[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'y_pred': y_pred}\n",
    "    print(f\"  {name:<40} MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}\")\n",
    "\n",
    "# Residual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "for ax, (name, res) in zip(axes.ravel(), deep_results.items()):\n",
    "    residuals_d = y - res['y_pred']\n",
    "    ax.scatter(y, residuals_d, alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Actual Target')\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R2={res[\"R2\"]:.3f}')\n",
    "    z = np.polyfit(y, residuals_d, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(y.min(), y.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'slope={z[0]:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Strategy 4: Transfer Learning Features - Residual Plots', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy4_deep_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: strategy4_deep_features.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41eaa12",
   "metadata": {},
   "source": [
    "## Overall Comparison Across All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b6ecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:32.986322Z",
     "iopub.status.busy": "2026-02-14T06:37:32.986244Z",
     "iopub.status.idle": "2026-02-14T06:37:33.164212Z",
     "shell.execute_reply": "2026-02-14T06:37:33.163830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect all results into one table\n",
    "all_results = {}\n",
    "\n",
    "# Baseline\n",
    "all_results['Lasso (baseline)'] = robust_results.get('Lasso (baseline)',\n",
    "                                   transform_results.get('Lasso (no transform)'))\n",
    "\n",
    "# Strategy 1: best robust\n",
    "for name in ['HuberRegressor (e=1.35)', 'HuberRegressor (e=1.1)', 'TheilSenRegressor']:\n",
    "    if name in robust_results:\n",
    "        all_results[f'S1: {name}'] = robust_results[name]\n",
    "\n",
    "# Strategy 2: best transform\n",
    "for name in ['Lasso + log(1+y)', 'Lasso + sqrt(y)', 'Ridge + log(1+y)']:\n",
    "    if name in transform_results:\n",
    "        all_results[f'S2: {name}'] = transform_results[name]\n",
    "\n",
    "# Strategy 3: best nonlinear\n",
    "for name in ['SVR (tuned)', 'XGBoost', 'GradientBoosting']:\n",
    "    if name in nonlinear_results:\n",
    "        all_results[f'S3: {name}'] = nonlinear_results[name]\n",
    "\n",
    "# Strategy 4: best deep\n",
    "for name in ['Ridge + Deep (PCA-20)', 'Ridge + Combined']:\n",
    "    if name in deep_results:\n",
    "        all_results[f'S4: {name}'] = deep_results[name]\n",
    "\n",
    "# Print sorted by MAE\n",
    "print(\"=\" * 75)\n",
    "print(\"GRAND COMPARISON - ALL STRATEGIES (LOO CV)\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Model':<45} {'MAE':>7} {'RMSE':>7} {'R2':>7}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1]['MAE'])\n",
    "for name, res in sorted_results:\n",
    "    marker = \" *\" if name == sorted_results[0][0] else \"\"\n",
    "    print(f\"  {name:<43} {res['MAE']:>7.2f} {res['RMSE']:>7.2f} {res['R2']:>7.3f}{marker}\")\n",
    "\n",
    "best_name = sorted_results[0][0]\n",
    "print(f\"\\n* Best model: {best_name}\")\n",
    "print(f\"  vs Lasso baseline: MAE improved by \"\n",
    "      f\"{all_results['Lasso (baseline)']['MAE'] - sorted_results[0][1]['MAE']:.2f} \"\n",
    "      f\"({(1 - sorted_results[0][1]['MAE']/all_results['Lasso (baseline)']['MAE'])*100:.1f}%)\")\n",
    "\n",
    "# Final visualization: best model vs baseline\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "baseline_pred = all_results['Lasso (baseline)']['y_pred']\n",
    "best_pred = sorted_results[0][1]['y_pred']\n",
    "\n",
    "for ax, (pred, title) in zip(axes, [(baseline_pred, 'Lasso (baseline)'),\n",
    "                                      (best_pred, best_name)]):\n",
    "    ax.scatter(y, pred, alpha=0.6, s=40, c='steelblue', edgecolors='white', linewidth=0.5)\n",
    "    lims = [min(y.min(), pred.min()) - 2, max(y.max(), pred.max()) + 2]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.7, label='Perfect')\n",
    "    mae = mean_absolute_error(y, pred)\n",
    "    r2 = r2_score(y, pred)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(f'{title}\\nMAE={mae:.2f}, R2={r2:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Baseline vs Best Model - Predicted vs Actual (LOO CV)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('grand_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: grand_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6kiax72ynzh",
   "metadata": {},
   "source": [
    "## Outlier Analysis: Visualizing the Most Anomalous Samples\n",
    "\n",
    "Identify samples with the largest residuals from the Lasso baseline and inspect their images to understand what makes them difficult to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7j9n04x1uic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:33.165354Z",
     "iopub.status.busy": "2026-02-14T06:37:33.165270Z",
     "iopub.status.idle": "2026-02-14T06:37:44.359331Z",
     "shell.execute_reply": "2026-02-14T06:37:44.358591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify outliers from Lasso baseline\n",
    "baseline_pred = robust_results['Lasso (baseline)']['y_pred']\n",
    "residuals_baseline = y - baseline_pred\n",
    "\n",
    "# Sort by absolute residual, pick top outliers\n",
    "abs_resid = np.abs(residuals_baseline)\n",
    "outlier_idx = np.argsort(abs_resid)[::-1]  # largest residual first\n",
    "\n",
    "n_outliers = 5\n",
    "print(\"Top outlier samples (largest |residual|):\")\n",
    "print(f\"{'Rank':<6} {'Idx':<6} {'Actual':<10} {'Predicted':<12} {'Residual':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for rank, idx in enumerate(outlier_idx[:n_outliers]):\n",
    "    print(f\"{rank+1:<6} {idx:<6} {y[idx]:<10} {baseline_pred[idx]:<12.1f} {residuals_baseline[idx]:<10.1f}\")\n",
    "\n",
    "# Also find a few \"normal\" samples for comparison\n",
    "normal_idx = np.argsort(abs_resid)[:3]\n",
    "print(f\"\\nBest-predicted samples (smallest |residual|) for comparison:\")\n",
    "for idx in normal_idx:\n",
    "    print(f\"  Idx={idx}, Actual={y[idx]}, Predicted={baseline_pred[idx]:.1f}, Residual={residuals_baseline[idx]:.1f}\")\n",
    "\n",
    "# --- Visualize outlier images (all 6 channels + RGB composite) ---\n",
    "channel_names = ['Ch0 (DNA?)', 'Ch1 (ER?)', 'Ch2 (RNA?)', 'Ch3 (AGP?)', 'Ch4 (Mito?)', 'Ch5 (BF?)']\n",
    "\n",
    "fig, axes = plt.subplots(n_outliers, 7, figsize=(28, 5 * n_outliers))\n",
    "\n",
    "for row, idx in enumerate(outlier_idx[:n_outliers]):\n",
    "    # 6 individual channels\n",
    "    for ch in range(6):\n",
    "        img_ch = images[idx, :, :, ch]\n",
    "        vmax = np.percentile(img_ch, 99.5)\n",
    "        axes[row, ch].imshow(img_ch, cmap='gray', vmin=0, vmax=max(vmax, 1))\n",
    "        axes[row, ch].set_title(f'{channel_names[ch]}', fontsize=9)\n",
    "        axes[row, ch].axis('off')\n",
    "    \n",
    "    # RGB composite (Ch0, Ch1, Ch2)\n",
    "    rgb = np.stack([images[idx,:,:,0], images[idx,:,:,1], images[idx,:,:,2]], axis=-1)\n",
    "    p995 = np.percentile(rgb, 99.5)\n",
    "    if p995 > 0:\n",
    "        rgb_scaled = (rgb.astype(float) / p995 * 255).clip(0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        rgb_scaled = rgb.astype(np.uint8)\n",
    "    axes[row, 6].imshow(rgb_scaled)\n",
    "    axes[row, 6].set_title('RGB (Ch0-2)', fontsize=9)\n",
    "    axes[row, 6].axis('off')\n",
    "    \n",
    "    # Row label\n",
    "    axes[row, 0].set_ylabel(\n",
    "        f'#{row+1} Idx={idx}\\nActual={y[idx]}, Pred={baseline_pred[idx]:.1f}\\nResid={residuals_baseline[idx]:.1f}',\n",
    "        fontsize=11, fontweight='bold', rotation=0, labelpad=120, va='center'\n",
    "    )\n",
    "\n",
    "plt.suptitle(f'Top {n_outliers} Outlier Images (Largest |Residual| from Lasso Baseline)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: outlier_images.png\")\n",
    "\n",
    "# --- Now show \"normal\" well-predicted samples for contrast ---\n",
    "fig2, axes2 = plt.subplots(len(normal_idx), 7, figsize=(28, 5 * len(normal_idx)))\n",
    "\n",
    "for row, idx in enumerate(normal_idx):\n",
    "    for ch in range(6):\n",
    "        img_ch = images[idx, :, :, ch]\n",
    "        vmax = np.percentile(img_ch, 99.5)\n",
    "        axes2[row, ch].imshow(img_ch, cmap='gray', vmin=0, vmax=max(vmax, 1))\n",
    "        axes2[row, ch].set_title(f'{channel_names[ch]}', fontsize=9)\n",
    "        axes2[row, ch].axis('off')\n",
    "    \n",
    "    rgb = np.stack([images[idx,:,:,0], images[idx,:,:,1], images[idx,:,:,2]], axis=-1)\n",
    "    p995 = np.percentile(rgb, 99.5)\n",
    "    if p995 > 0:\n",
    "        rgb_scaled = (rgb.astype(float) / p995 * 255).clip(0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        rgb_scaled = rgb.astype(np.uint8)\n",
    "    axes2[row, 6].imshow(rgb_scaled)\n",
    "    axes2[row, 6].set_title('RGB (Ch0-2)', fontsize=9)\n",
    "    axes2[row, 6].axis('off')\n",
    "    \n",
    "    axes2[row, 0].set_ylabel(\n",
    "        f'Idx={idx}\\nActual={y[idx]}, Pred={baseline_pred[idx]:.1f}\\nResid={residuals_baseline[idx]:.1f}',\n",
    "        fontsize=11, fontweight='bold', rotation=0, labelpad=120, va='center'\n",
    "    )\n",
    "\n",
    "plt.suptitle('Well-Predicted Samples (Smallest |Residual|) \u2014 For Comparison', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('normal_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: normal_images.png\")\n",
    "\n",
    "# --- Residual plot with outliers highlighted ---\n",
    "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "ax3.scatter(y, residuals_baseline, alpha=0.5, s=40, c='steelblue', edgecolors='white', linewidth=0.5, label='Normal')\n",
    "ax3.scatter(y[outlier_idx[:n_outliers]], residuals_baseline[outlier_idx[:n_outliers]], \n",
    "            s=120, c='red', edgecolors='black', linewidth=1, zorder=5, label='Top 5 outliers')\n",
    "for rank, idx in enumerate(outlier_idx[:n_outliers]):\n",
    "    ax3.annotate(f'#{rank+1} (idx={idx})', (y[idx], residuals_baseline[idx]),\n",
    "                 textcoords=\"offset points\", xytext=(8, 8), fontsize=9, color='red', fontweight='bold')\n",
    "ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax3.set_xlabel('Actual Target')\n",
    "ax3.set_ylabel('Residual (Actual - Predicted)')\n",
    "ax3.set_title('Residual Plot with Outliers Highlighted')\n",
    "ax3.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_outliers_highlighted.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: residual_outliers_highlighted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3kwdltsnsmp",
   "metadata": {},
   "source": [
    "## Recommended Fix: HuberRegressor + sqrt Target Transformation\n",
    "\n",
    "Combine robust regression (handles outliers like Idx=63) with sqrt target transformation (stabilizes variance across the range). Then check if the residual bias (slope) is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m68kkmsr3y",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:44.360913Z",
     "iopub.status.busy": "2026-02-14T06:37:44.360800Z",
     "iopub.status.idle": "2026-02-14T06:37:51.937870Z",
     "shell.execute_reply": "2026-02-14T06:37:51.937386Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# Define candidate models\n",
    "fix_models = {\n",
    "    'Lasso (baseline)': Lasso(alpha=1.0),\n",
    "    'Huber + sqrt(y)': TransformedTargetRegressor(\n",
    "        regressor=HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "    'Huber + log(1+y)': TransformedTargetRegressor(\n",
    "        regressor=HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500),\n",
    "        func=np.log1p, inverse_func=np.expm1\n",
    "    ),\n",
    "    'Huber (no transform)': HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500),\n",
    "    'Ridge + sqrt(y)': TransformedTargetRegressor(\n",
    "        regressor=Ridge(alpha=10.0),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "}\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "fix_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HUBER + SQRT ANALYSIS \u2014 LOO CV\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Model':<30} {'MAE':>7} {'RMSE':>7} {'R2':>7} {'Resid Slope':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model in fix_models.items():\n",
    "    y_pred = cross_val_predict(model, X_scaled, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    resid = y - y_pred\n",
    "    slope = np.polyfit(y, resid, 1)[0]\n",
    "    fix_results[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'y_pred': y_pred, 'slope': slope}\n",
    "    print(f\"  {name:<30} {mae:>7.2f} {rmse:>7.2f} {r2:>7.3f} {slope:>12.4f}\")\n",
    "\n",
    "print(f\"\\n  (Slope closer to 0 = less systematic bias)\")\n",
    "\n",
    "# --- Residual comparison: 2x3 grid ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes_flat = axes.ravel()\n",
    "\n",
    "for i, (name, res) in enumerate(fix_results.items()):\n",
    "    if i >= 6:\n",
    "        break\n",
    "    ax = axes_flat[i]\n",
    "    resid = y - res['y_pred']\n",
    "    \n",
    "    ax.scatter(y, resid, alpha=0.6, s=40, edgecolors='white', linewidth=0.5,\n",
    "               c='coral' if name == 'Lasso (baseline)' else 'steelblue')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Actual Target')\n",
    "    ax.set_ylabel('Residual')\n",
    "    \n",
    "    # Trend line\n",
    "    z = np.polyfit(y, resid, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(y.min(), y.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, linewidth=2, label=f'slope={z[0]:.3f}')\n",
    "    \n",
    "    # Highlight outliers\n",
    "    abs_r = np.abs(resid)\n",
    "    top3 = np.argsort(abs_r)[-3:]\n",
    "    ax.scatter(y[top3], resid[top3], s=100, c='red', edgecolors='black', linewidth=1, zorder=5)\n",
    "    \n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R\u00b2={res[\"R2\"]:.3f}, slope={res[\"slope\"]:.3f}', fontsize=11)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "# Hide unused subplot if any\n",
    "for j in range(len(fix_results), 6):\n",
    "    axes_flat[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Residual Comparison: Baseline vs HuberRegressor + Target Transform', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('huber_sqrt_fix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: huber_sqrt_fix.png\")\n",
    "\n",
    "# --- Predicted vs Actual: best fix vs baseline ---\n",
    "best_fix_name = min(\n",
    "    [(k, v) for k, v in fix_results.items() if k != 'Lasso (baseline)'],\n",
    "    key=lambda x: abs(x[1]['slope'])\n",
    ")[0]\n",
    "\n",
    "fig2, axes2 = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "for ax, name in zip(axes2, ['Lasso (baseline)', best_fix_name, 'Huber + sqrt(y)']):\n",
    "    res = fix_results[name]\n",
    "    ax.scatter(y, res['y_pred'], alpha=0.6, s=40, c='steelblue', edgecolors='white', linewidth=0.5)\n",
    "    lims = [min(y.min(), res['y_pred'].min()) - 2, max(y.max(), res['y_pred'].max()) + 2]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.7, label='Perfect')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R\u00b2={res[\"R2\"]:.3f}, slope={res[\"slope\"]:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Predicted vs Actual: Baseline \u2192 Fixed', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('huber_sqrt_pred_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: huber_sqrt_pred_vs_actual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fv4nx1sv3sn",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Systematic Optimization \u2014 Sqrt + Huber + Feature Selection\n",
    "\n",
    "Three-pronged approach to eliminate the residual bias:\n",
    "- **A. sqrt(y) transformation**: compress count-data variance, make outliers less extreme\n",
    "- **B. HuberRegressor**: linear penalty beyond threshold, refuse to \"sacrifice the majority for one outlier\"\n",
    "- **C. Feature Selection (SelectKBest)**: remove redundant features, reduce noise\n",
    "\n",
    "## Step 1: Quantify Outlier Features\n",
    "\n",
    "Before modeling fixes, examine *why* the outliers are outliers \u2014 which features are abnormal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7v7m0wubq",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:51.939339Z",
     "iopub.status.busy": "2026-02-14T06:37:51.939249Z",
     "iopub.status.idle": "2026-02-14T06:37:52.163823Z",
     "shell.execute_reply": "2026-02-14T06:37:52.163476Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1: Quantify Outlier Features\n",
    "# ============================================================\n",
    "# Get outlier indices from Lasso baseline\n",
    "baseline_pred = robust_results['Lasso (baseline)']['y_pred']\n",
    "residuals_baseline = y - baseline_pred\n",
    "abs_resid = np.abs(residuals_baseline)\n",
    "outlier_indices = np.argsort(abs_resid)[::-1][:5]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OUTLIER FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute z-scores for all features\n",
    "from scipy.stats import zscore\n",
    "feature_z = pd.DataFrame(zscore(feature_df), columns=feature_df.columns)\n",
    "\n",
    "for idx in outlier_indices[:3]:  # Top 3 outliers\n",
    "    print(f\"\\n--- Sample Idx={idx} | Actual={y[idx]}, Predicted={baseline_pred[idx]:.1f}, Residual={residuals_baseline[idx]:.1f} ---\")\n",
    "    z_row = feature_z.iloc[idx]\n",
    "    # Features where this sample is >2 std from mean\n",
    "    extreme_feats = z_row[z_row.abs() > 2.0].sort_values(key=abs, ascending=False)\n",
    "    if len(extreme_feats) > 0:\n",
    "        print(f\"  Features with |z-score| > 2.0 ({len(extreme_feats)} features):\")\n",
    "        for feat, zval in extreme_feats.head(10).items():\n",
    "            actual_val = feature_df.iloc[idx][feat]\n",
    "            pop_mean = feature_df[feat].mean()\n",
    "            print(f\"    {feat:<30} z={zval:>+6.2f}  (value={actual_val:.2f}, mean={pop_mean:.2f})\")\n",
    "    else:\n",
    "        print(\"  No features with |z-score| > 2.0\")\n",
    "\n",
    "# Heatmap: top outliers vs population for key features\n",
    "top_corr_feats = correlations.head(15).index.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "compare_idx = list(outlier_indices[:5]) + list(np.argsort(abs_resid)[:3])  # outliers + normals\n",
    "labels = [f'OUT idx={i}\\ny={y[i]},r={residuals_baseline[i]:.0f}' for i in outlier_indices[:5]] + \\\n",
    "         [f'GOOD idx={i}\\ny={y[i]},r={residuals_baseline[i]:.0f}' for i in np.argsort(abs_resid)[:3]]\n",
    "\n",
    "data_for_heatmap = feature_z.iloc[compare_idx][top_corr_feats]\n",
    "sns.heatmap(data_for_heatmap, annot=True, fmt='.1f', cmap='RdBu_r', center=0,\n",
    "            xticklabels=top_corr_feats, yticklabels=labels, ax=ax,\n",
    "            cbar_kws={'label': 'Z-score'})\n",
    "ax.set_title('Feature Z-scores: Outliers vs Well-Predicted Samples\\n(Top 15 correlated features)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_feature_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: outlier_feature_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ittav4bqtrt",
   "metadata": {},
   "source": [
    "## Step 2: Build Optimized Pipeline \u2014 Sqrt + Huber + SelectKBest\n",
    "\n",
    "Systematically compare every combination to find the best fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3oxakhc2jcp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:52.165013Z",
     "iopub.status.busy": "2026-02-14T06:37:52.164933Z",
     "iopub.status.idle": "2026-02-14T06:37:57.227821Z",
     "shell.execute_reply": "2026-02-14T06:37:57.227513Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2: Systematic comparison of all optimization combos\n",
    "# ============================================================\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# --- C. Feature Selection: find optimal k ---\n",
    "print(\"Finding optimal k for SelectKBest...\")\n",
    "k_scores = {}\n",
    "for k in [10, 15, 20, 30, 50, 85]:\n",
    "    if k > X_scaled.shape[1]:\n",
    "        continue\n",
    "    pipe = Pipeline([\n",
    "        ('select', SelectKBest(f_regression, k=k)),\n",
    "        ('model', Lasso(alpha=1.0))\n",
    "    ])\n",
    "    y_pred = cross_val_predict(pipe, X_scaled, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    k_scores[k] = mae\n",
    "    print(f\"  k={k:<4} MAE={mae:.2f}\")\n",
    "best_k = min(k_scores, key=k_scores.get)\n",
    "print(f\"  Best k={best_k} (MAE={k_scores[best_k]:.2f})\")\n",
    "\n",
    "# --- Full comparison: all combos of {Lasso, Huber} x {raw, sqrt} x {all, selectk} ---\n",
    "combos = {\n",
    "    # Baseline\n",
    "    'Lasso (original)': Pipeline([\n",
    "        ('model', Lasso(alpha=1.0))\n",
    "    ]),\n",
    "    # A: sqrt transform only\n",
    "    'Lasso + sqrt(y)': TransformedTargetRegressor(\n",
    "        regressor=Lasso(alpha=1.0),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "    # B: Huber only\n",
    "    'Huber': Pipeline([\n",
    "        ('model', HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500))\n",
    "    ]),\n",
    "    # C: Feature selection only\n",
    "    f'Lasso + SelectK(k={best_k})': Pipeline([\n",
    "        ('select', SelectKBest(f_regression, k=best_k)),\n",
    "        ('model', Lasso(alpha=1.0))\n",
    "    ]),\n",
    "    # A+B: Huber + sqrt\n",
    "    'Huber + sqrt(y)': TransformedTargetRegressor(\n",
    "        regressor=HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "    # A+C: Lasso + sqrt + SelectK\n",
    "    f'Lasso + sqrt + SelectK({best_k})': TransformedTargetRegressor(\n",
    "        regressor=Pipeline([\n",
    "            ('select', SelectKBest(f_regression, k=best_k)),\n",
    "            ('model', Lasso(alpha=1.0))\n",
    "        ]),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "    # A+B+C: Huber + sqrt + SelectK (full combo)\n",
    "    f'Huber + sqrt + SelectK({best_k})': TransformedTargetRegressor(\n",
    "        regressor=Pipeline([\n",
    "            ('select', SelectKBest(f_regression, k=best_k)),\n",
    "            ('model', HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500))\n",
    "        ]),\n",
    "        func=np.sqrt, inverse_func=np.square\n",
    "    ),\n",
    "    # B+C: Huber + SelectK\n",
    "    f'Huber + SelectK({best_k})': Pipeline([\n",
    "        ('select', SelectKBest(f_regression, k=best_k)),\n",
    "        ('model', HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=500))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FULL OPTIMIZATION COMPARISON \u2014 LOO CV\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  {'Model':<38} {'MAE':>6} {'RMSE':>6} {'R2':>6} {'Slope':>7} {'Max|r|':>7}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "combo_results = {}\n",
    "for name, model in combos.items():\n",
    "    y_pred = cross_val_predict(model, X_scaled, y, cv=loo)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    resid = y - y_pred\n",
    "    slope = np.polyfit(y, resid, 1)[0]\n",
    "    max_resid = np.max(np.abs(resid))\n",
    "    combo_results[name] = {\n",
    "        'MAE': mae, 'RMSE': rmse, 'R2': r2,\n",
    "        'y_pred': y_pred, 'slope': slope, 'max_resid': max_resid\n",
    "    }\n",
    "    print(f\"  {name:<38} {mae:>6.2f} {rmse:>6.2f} {r2:>6.3f} {slope:>+7.3f} {max_resid:>7.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ocnm5phbi",
   "metadata": {},
   "source": [
    "## Step 3: Visual Diagnosis \u2014 Has the Bias Been Fixed?\n",
    "\n",
    "The acid test: are residuals now **randomly scattered around 0** with no slope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ujao3d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T06:37:57.229124Z",
     "iopub.status.busy": "2026-02-14T06:37:57.229056Z",
     "iopub.status.idle": "2026-02-14T06:37:57.888698Z",
     "shell.execute_reply": "2026-02-14T06:37:57.888175Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3: Head-to-head residual plot \u2014 Original vs Optimized\n",
    "# ============================================================\n",
    "\n",
    "# Pick key models to compare\n",
    "show_models = ['Lasso (original)', 'Huber', 'Huber + sqrt(y)']\n",
    "# Add best SelectK combo\n",
    "selectk_models = [k for k in combo_results if 'SelectK' in k]\n",
    "if selectk_models:\n",
    "    best_sk = min(selectk_models, key=lambda k: abs(combo_results[k]['slope']))\n",
    "    show_models.append(best_sk)\n",
    "\n",
    "n_show = len(show_models)\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(7 * n_show, 12))\n",
    "\n",
    "for col, name in enumerate(show_models):\n",
    "    res = combo_results[name]\n",
    "    pred = res['y_pred']\n",
    "    resid = y - pred\n",
    "    is_baseline = (name == 'Lasso (original)')\n",
    "    color = 'coral' if is_baseline else 'steelblue'\n",
    "    \n",
    "    # Row 1: Predicted vs Actual\n",
    "    ax1 = axes[0, col]\n",
    "    ax1.scatter(y, pred, alpha=0.6, s=40, c=color, edgecolors='white', linewidth=0.5)\n",
    "    lims = [min(y.min(), pred.min()) - 3, max(y.max(), pred.max()) + 3]\n",
    "    ax1.plot(lims, lims, 'k--', alpha=0.4, label='Perfect')\n",
    "    ax1.set_xlabel('Actual')\n",
    "    ax1.set_ylabel('Predicted')\n",
    "    ax1.set_title(f'{name}\\nMAE={res[\"MAE\"]:.2f}, R\u00b2={res[\"R2\"]:.3f}', fontsize=11)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Highlight outlier idx=63\n",
    "    if 63 in range(len(y)):\n",
    "        ax1.scatter(y[63], pred[63], s=150, c='red', edgecolors='black', linewidth=2, zorder=5, marker='*')\n",
    "        ax1.annotate('idx=63', (y[63], pred[63]), textcoords=\"offset points\",\n",
    "                     xytext=(10, -10), fontsize=9, color='red', fontweight='bold')\n",
    "    \n",
    "    # Row 2: Residual plot\n",
    "    ax2 = axes[1, col]\n",
    "    ax2.scatter(y, resid, alpha=0.6, s=40, c=color, edgecolors='white', linewidth=0.5)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.set_xlabel('Actual Target')\n",
    "    ax2.set_ylabel('Residual')\n",
    "    \n",
    "    # Trend line\n",
    "    z = np.polyfit(y, resid, 1)\n",
    "    p_line = np.poly1d(z)\n",
    "    x_line = np.linspace(y.min(), y.max(), 100)\n",
    "    ax2.plot(x_line, p_line(x_line), 'r-', alpha=0.8, linewidth=2.5, label=f'slope={z[0]:+.3f}')\n",
    "    ax2.fill_between(x_line, p_line(x_line) - 0.5, p_line(x_line) + 0.5, alpha=0.1, color='red')\n",
    "    \n",
    "    # Highlight outliers\n",
    "    top3 = np.argsort(np.abs(resid))[-3:]\n",
    "    ax2.scatter(y[top3], resid[top3], s=100, c='red', edgecolors='black', linewidth=1.5, zorder=5)\n",
    "    \n",
    "    verdict = \"BIASED\" if abs(z[0]) > 0.15 else (\"IMPROVED\" if abs(z[0]) > 0.08 else \"GOOD\")\n",
    "    ax2.set_title(f'Residuals: slope={z[0]:+.3f}, max|r|={res[\"max_resid\"]:.1f}\\n[{verdict}]',\n",
    "                  fontsize=11, color='red' if verdict == \"BIASED\" else ('orange' if verdict == \"IMPROVED\" else 'green'))\n",
    "    ax2.legend(fontsize=11)\n",
    "\n",
    "plt.suptitle('Original vs Optimized: Predicted-vs-Actual (top) & Residuals (bottom)', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: optimization_comparison.png\")\n",
    "\n",
    "# --- Summary verdict ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZATION VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "baseline = combo_results['Lasso (original)']\n",
    "for name in show_models:\n",
    "    res = combo_results[name]\n",
    "    slope_change = abs(baseline['slope']) - abs(res['slope'])\n",
    "    maxr_change = baseline['max_resid'] - res['max_resid']\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Slope:     {baseline['slope']:+.3f} -> {res['slope']:+.3f}  (bias {'reduced' if slope_change > 0 else 'increased'} by {abs(slope_change):.3f})\")\n",
    "    print(f\"    Max|resid|: {baseline['max_resid']:.1f} -> {res['max_resid']:.1f}  ({'better' if maxr_change > 0 else 'worse'} by {abs(maxr_change):.1f})\")\n",
    "    print(f\"    MAE:       {baseline['MAE']:.2f} -> {res['MAE']:.2f}\")\n",
    "    print(f\"    R\u00b2:        {baseline['R2']:.3f} -> {res['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "og9w9qkaqn",
   "source": "**Conclusion:**\n\n1. **Best Model:** The combination of manual features (especially nuclei counts) with a Robust Regressor (Huber) and target transformation (sqrt) provided the most reliable results ($R^2 \\approx 0.88$).\n\n2. **Key Findings:** Channel 0 (DNA/Nuclei) is the strongest predictor of the target value, confirming that the targets are likely related to cell counts.\n\n3. **Addressing Bias:** While traditional Lasso showed a high $R^2$, it suffered from systematic bias. The robust modeling approach successfully flattened the residual plot, making the model generalize better across all target ranges.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}